{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7186d5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi nltk transformers langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cf6e7d5-be55-4f60-af58-74e87c809ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab72ad81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk \n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01aa974a-a9f1-4990-909a-a6ed737e2fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Menghilangkan karakter non-alfanumerik dan mengganti beberapa karakter\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Opsi tambahan: mengganti newline dengan spasi, dll.\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba99a4af-d5aa-4667-9c53-82ac608aa528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inisialisasi Tokenizer mBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "971e9cd1-cf88-4ca5-9b0b-5b1df40d3d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(text, max_length=700, stride=50):\n",
    "    # Konversi teks ke token dan inisialisasi segmen\n",
    "    initial_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    total_tokens = len(initial_tokens)\n",
    "    window_segments = []\n",
    "\n",
    "    if total_tokens <= max_length:\n",
    "        # Jika total token kurang dari max_length, tidak perlu sliding window\n",
    "        return [tokenizer.encode(text, add_special_tokens=True)]\n",
    "\n",
    "    for i in range(0, total_tokens, stride):\n",
    "        # Memilih segmen token dengan mempertimbangkan token spesial\n",
    "        window_segment = [tokenizer.cls_token_id] + initial_tokens[i:i + max_length - 2] + [tokenizer.sep_token_id]\n",
    "        window_segments.append(window_segment)\n",
    "\n",
    "    return window_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5bc89ac-f6d7-45e4-a0e5-9b4cebd917b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fungsi untuk menggabungkan token menjadi string\n",
    "def tokens_to_string(tokens):\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60f45589-fdad-4149-9745-3326115eb480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memuat dataset\n",
    "df = pd.read_csv('Merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6c98dce-40d1-4eb9-9afc-8c8d3098c757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Pembersihan teks\n",
    "df['cleaned_OCR_text'] = df['OCR_Text'].apply(clean_text)\n",
    "# Sliding window tokenization\n",
    "df['tokenized_segments'] = df['cleaned_OCR_text'].apply(sliding_window_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb40969e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Menggabungkan segmen teks yang telah ditokenisasi\n",
    "df['processed_text'] = df['tokenized_segments'].apply(lambda segs: ' '.join([tokens_to_string(seg) for seg in segs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fee352a-8af1-4f4a-8cb9-fea6047417c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87d3ba-15db-498d-bdcb-6d25c785797a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
